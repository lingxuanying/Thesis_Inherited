# -*- coding: utf-8 -*-
"""實驗重現_Fall40

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gI6zXtQULk8vsdw9nr-1EDsTmAr2ec1d
"""

import time

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader

from sklearn.preprocessing import MinMaxScaler
import pickle
import sklearn
print(torch.__version__)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sn
import random
from math import e
import math
import sys

batch_size = 24
feature_num = 9 #add similarity
record_num = 4

def adjust_input(input, zero_list):
  for i in range(len(input)):
    while(len(input[i]) < record_num):
      input[i].append(zero_list)
  return np.array(input)

def zero_norm(data):
  mean, std, var = torch.mean(data), torch.std(data), torch.var(data)
  data = (data-mean)/std
  return data

def cal_x_len(data):
  l = []
  for i in data:
    l.append(len(i))
  return np.array(l)

def preprocess_temp(data):
  for i in range(len(data)):
    for j in range(24):
      if(data[i][j] >= 27):
        data[i][j] = 0
      else:
        #data[i][j] = 1 / (math.log(27 - data[i][j] + 1) + 1)
        data[i][j] = math.exp((27 - data[i][j]) / 8)
  return data

"""## Load Data"""

# Get the non-sequential and temperature data
training_file = sys.argv[1] + 'train_file.pickle'
validation_file = sys.argv[1] + 'valid_file.pickle'
testing_file = sys.argv[1]+'test_file.pickle'
train = pickle.load(open(training_file, 'rb'))
validate = pickle.load(open(validation_file, 'rb'))
test = pickle.load(open(testing_file, 'rb'))

zero_list = -1

# non-sequential
info_train = adjust_input(train[2], zero_list)
info_train = np.array(zero_norm(torch.from_numpy(info_train))) # zero_mean
#info_train = info_train.T[:9].T
info_validate = adjust_input(validate[2], zero_list)
info_validate = np.array(zero_norm(torch.from_numpy(info_validate)))
#info_validate = info_validate.T[:9].T
info_test = adjust_input(test[2], zero_list)
info_test = np.array(zero_norm(torch.from_numpy(info_test)))
#info_test = info_test.T[:9].T


# temperature
temp_train = adjust_input(train[4], zero_list)
#temp_train = preprocess_temp(temp_train)
#temp_train = np.array(zero_norm(torch.from_numpy(temp_train))) # zero_mean

temp_validate = adjust_input(validate[4], zero_list)
#temp_validate = preprocess_temp(temp_validate)
#temp_validate = np.array(zero_norm(torch.from_numpy(temp_validate)))
temp_test = adjust_input(test[4], zero_list)
#temp_test = preprocess_temp(temp_test)
#temp_test = np.array(zero_norm(torch.from_numpy(temp_test)))

info_train

#%% Load Data
output_file = sys.argv[3]
training_file = sys.argv[1] + 'train_file.pickle'
validation_file = sys.argv[1] + 'valid_file.pickle'
testing_file = sys.argv[1]+'test_file.pickle'


train = pickle.load(open(training_file, 'rb'))
validate = pickle.load(open(validation_file, 'rb'))
test = pickle.load(open(testing_file, 'rb'))

zero_list = [-1] * feature_num

# feature's length (for last embedding)
l_train = cal_x_len(train[0])
l_validate = cal_x_len(validate[0])
l_test = cal_x_len(test[0])

# sequential variable
x_train = adjust_input(train[0], zero_list)
x_validate = adjust_input(validate[0], zero_list)
x_test = adjust_input(test[0], zero_list)

_, _, input_dimension = x_train.shape

# time step
zero_list = 480

t_train = adjust_input(train[3], zero_list)
t_train = np.array(zero_norm(torch.from_numpy(t_train))) # zero_mean

t_validate = adjust_input(validate[3], zero_list)
t_validate = np.array(zero_norm(torch.from_numpy(t_validate)))

t_test = adjust_input(test[3], zero_list)
t_test = np.array(zero_norm(torch.from_numpy(t_test)))

#%% Load unreliable table
u_training_file = sys.argv[2]+'train_file.pickle'
u_validation_file = sys.argv[2]+'valid_file.pickle'
u_testing_file = sys.argv[2]+'test_file.pickle'
u_train = pickle.load(open(u_training_file, 'rb'))
u_validate = pickle.load(open(u_validation_file, 'rb'))
u_test = pickle.load(open(u_testing_file, 'rb'))

'''
# add 0 to similarity
for i in range(len(u_train[0])):
  for j in range(len(u_train[0][i])):
    u_train[0][i][j].append(0)

for i in range(len(u_validate[0])):
  for j in range(len(u_validate[0][i])):
    u_validate[0][i][j].append(0)

for i in range(len(u_test[0])):
  for j in range(len(u_test[0][i])):
    u_test[0][i][j].append(0)
'''
# unreliable
one_list = [0] * feature_num
u_train = adjust_input(u_train, one_list)
u_validate = adjust_input(u_validate, one_list)
u_test = adjust_input(u_test, one_list)

# label
y_train = np.array(train[1]) # (1397, )
y_validate = np.array(validate[1]) # (183, )
y_test = np.array(test[1]) # (170, )

y_train = np.expand_dims(y_train, axis=1)
y_validate = np.expand_dims(y_validate, axis=1)
y_test = np.expand_dims(y_test, axis=1)

#%% is_GPU
is_cuda = torch.cuda.is_available()

# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
if is_cuda:
    device = torch.device("cuda")
else:
    device = torch.device("cpu")

"""## Variables
x_train/validate/test: sequential<br>
l_train/validate/test: sequential length<br>
info_train/validate/test: non-sequqntial<br>
t_train/validate/test: time step<br>
u_train/validate/test: unrealiable value<br>
temp_train/validate/test: temperature

## GRU model
"""

#%% GRU function
Dense = torch.nn.Linear
LayerNorm = torch.nn.LayerNorm
class TimeDistributed(nn.Module):
    def __init__(self, module, batch_first):
        super(TimeDistributed, self).__init__()
        self.module = module
        self.batch_first = batch_first

    def forward(self, input_seq):
        assert len(input_seq.size()) > 2

        reshaped_input = input_seq.contihuous().view(-1, input_seq.size(-1))
        output = self.module(reshaped_input)

        if self.batch_first:
            output = output.contihuous().view(input_seq.siez(0), -1, output.size(-1))
        else:
            output = output.contihuous().view(-1, input_seq.siez(0), output.size(-1))
        return output

def linear_layer(input_size, size, activation=None, use_time_distributed=False, use_bias=True):
    linear = torch.nn.Linear(input_size, size, bias=use_bias)
    if use_time_distributed:
        linear = TimeDistributed(linear)
    return linear

def apply_gating_layer(x, hidden_layer_size, dropout_rate=None, use_time_distributed=True, activation=None):
    if dropout_rate is not None:
        x = torch.nn.Dropout(dropout_rate)(x)

    if use_time_distributed:
        activation_layer = TimeDistributed(
            torch.nn.Linear(x.shape[-1], hidden_layer_size))(
            x)
        gated_layer = TimeDistributed(
            torch.nn.Linear(x.shape[-1], hidden_layer_size))(
            x)
    else:
        activation_layer = torch.nn.Linear(
            x.shape[-1], hidden_layer_size)(
            x)
        gated_layer = torch.nn.Linear(
            x.shape[-1], hidden_layer_size)(
            x)

    return torch.mul(activation_layer, gated_layer), gated_layer

def add_and_norm(x, y):
    tmp = x + y
    tmp = LayerNorm(tmp.shape)(tmp)
    return tmp

def gated_residual_network(x, hidden_layer_size, output_size=None, dropout_rate=None, use_time_distributed=True, additional_context=None, return_gate=False):
    # Setup skip connection
    if output_size is None:
        output_size = hidden_layer_size
        skip = x
    else:
        linear = Dense(x.shape[-1], output_size)
        if use_time_distributed:
            linear = TimeDistributed(linear)
        skip = linear(x)

    # Apply feedforward network
    hidden = linear_layer(
        x.shape[-1],
        hidden_layer_size,
        activation=None,
        use_time_distributed=use_time_distributed)(
        x)

    hidden = torch.nn.ELU()(hidden)
    hidden = linear_layer(
        hidden_layer_size,
        hidden_layer_size,
        activation=None,
        use_time_distributed=use_time_distributed)(
        hidden)

    gating_layer, gate = apply_gating_layer(
        hidden,
        output_size,
        dropout_rate=dropout_rate,
        use_time_distributed=use_time_distributed,
        activation=None)

    if return_gate:
        return add_and_norm(skip, gating_layer), gate
    else:
        # print('skip: ', skip.shape)
        # print('gating_layer: ', gating_layer.shape)
        # print('add_and_norm(skip, gating_layer)', add_and_norm(skip, gating_layer).shape)
        return add_and_norm(skip, gating_layer)

def static_combine_and_mask(embedding):
    # Add temporal features
    _, num_time, num_static = embedding.shape

    flatten = torch.nn.Flatten()(embedding)

    # Nonlinear transformation with gated residual network.
    mlp_outputs = gated_residual_network(
        flatten,
        hidden_layer_size=5,
        output_size=num_static,
        dropout_rate=0.2,
        use_time_distributed=False,
        additional_context=None)
    sparse_weights = torch.nn.Softmax()(mlp_outputs)
    sparse_weights = torch.unsqueeze(sparse_weights, -1) # (24, 9, 1) 非時序特徵權重

    trans_emb_list = torch.tensor([])
    for i in range(num_static):
        e = gated_residual_network(
            embedding[:, :, i:i + 1],
            hidden_layer_size=1,
            dropout_rate=0.2,
            use_time_distributed=False)
        trans_emb_list = torch.cat((trans_emb_list, e), 2)

    transformed_embedding = torch.permute(trans_emb_list, (0, 2, 1))
    combined = torch.mul(sparse_weights, transformed_embedding)
    combined = torch.permute(combined, (0, 2, 1))
    static_vec = torch.sum(combined, 1)

    return combined, sparse_weights
class GRUNet(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):
        super(GRUNet, self).__init__()
        self.hidden_dim = hidden_dim
        self.n_layers = n_layers


        self.gru = nn.GRU(9, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
        self.batch_norm = nn.BatchNorm1d(input_dim)
        self.dropout = nn.Dropout(p=0.5)

        self.layer_t = nn.Linear(record_num, hidden_dim)
        self.layer_c = nn.Linear(input_dim, input_dim, bias=False)
        self.layer_adjust = nn.Linear(hidden_dim, hidden_dim)
        self.gru_adjust = nn.GRU(hidden_dim + 35, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)
        self.layer_final = nn.Linear(hidden_dim, 24)
        self.layer_combine = nn.Linear(24, 1)
        self.softmax = nn.Softmax(dim=0)

        self.layer_info = nn.Linear(9, 18)
        self.temp_encoder = nn.TransformerEncoderLayer(d_model=24, nhead=6)
        self.transformer_encoder = nn.TransformerEncoder(self.temp_encoder, num_layers=1)
        self.layer_temp = nn.Linear(24, 1)
        self.layer_upsample = nn.Linear(9, 9)

        #self.gru_temp = nn.GRU(batch_size, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)



    def forward(self, x, h, c, arpha_h, final_h, t, l, info, temp):
        decay_t = 1 / torch.log(e + t)
        decay_t = self.sigmoid(decay_t)

        #final_h = final_h * self.layer_t(decay_t)

        #print(decay_t.shape)
        #decay_temp = 1 / torch.log(27 - temp[:, 22])

        # unreliablility-aware attention
        out = self.batch_norm(torch.permute(x, (0, 2, 1)))
        out = torch.permute(out, (0, 2, 1))
        #arpha = self.sigmoid(c)
        #arpha = self.layer_upsample(c)
        out = self.layer_upsample(out)
        #unreliable_attention = arpha * out

        # Symptom-aware attention
        #c_01 = torch.floor(c)
        #c_01 = self.layer_upsample(c_01)
        decay_t = torch.unsqueeze(decay_t, 2)
        x_deep, h_deep = self.gru(out * decay_t, h) #左T-GRU


        #print(arpha_h.shape)
        #print(arpha.shape)

        #print(decay_t.shape)
        arpha_deep, arpha_h_deep = self.gru(decay_t * c, arpha_h) # 右T-GRU

        #print(c.shape)
        symptom_attention = x_deep * arpha_deep

        x_adjust = self.relu(self.layer_adjust(symptom_attention))

        # 將non-sequential由(24, 9)變為(24, 4, 9)與x_adjust(24, 4, 256)接在一起過最後的GRU

        info = info.unsqueeze(1)
        info = torch.cat((info, info, info, info), 1)
        static_encoder, static_weights = static_combine_and_mask(info)

        #print(temp[:, 22])
        #temp_x = torch.log(((27 - temp[:, 22])/4) ** 1.273)
        temp = self.transformer_encoder(temp)
        temp = torch.unsqueeze(temp, 0)
        temp = self.layer_temp(temp)

        #print(temp.shape)
        #Temperature
        final_h = temp * final_h

        #temp = temp.unsqueeze(1)
        #temp = torch.cat((temp, temp, temp, temp), 1)
        #temp, temp_h = self.gru_temp(temp, temp_h)
        # 非時序性
        out, h_out = self.gru_adjust(torch.cat((x_adjust, static_encoder), 2), final_h)
        #out, h_out = self.gru_adjust(x_adjust, final_h)
        out = self.dropout(out)


        # Combine all the features
        out = self.layer_final(out[:,-1])
        f = self.sigmoid(self.layer_combine(out)) # 36->1

        return f, h_deep, arpha_h_deep, h_out

    def init_hidden(self, batch_size):
        weight = next(self.parameters()).data
        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)
        return hidden

"""## Train function"""

def train(x_train, learn_rate, hidden_dim=256, EPOCHS=25, model_type="GRU"):
    input_dim = feature_num

    #input_dim = next(iter(train_loader))[0].shape[2]
    output_dim = 1
    n_layers = 1
    # Instantiating the models
    model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)
    model.to(device)

    # Defining loss function and optimizer
    criterion = nn.BCELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)
    lambda1 = lambda epoch: 0.95 ** epoch if epoch > 5 else 1
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda1)

    total_accuracy = []
    total_precision = []
    total_recall = []
    total_f1 = []
    total_roc_auc = []
    total_train_cost = []
    total_valid_cost = []
    best_accuracy = 0.0
    best_epoch = 0
    best_train_cost = 100000000.0
    best_valid_cost = 100000000.0

    print("Starting Training of {} model".format(model_type))
    # Start training loop
    for epoch in range(1,EPOCHS+1):
        n_batches = int(np.ceil(float(len(x_train)) / float(batch_size)))
        h = model.init_hidden(batch_size)
        #h = torch.rand(1, batch_size, hidden_dim)
        arpha_h = model.init_hidden(batch_size)
        #print(arpha_h.shape)
        final_h = model.init_hidden(batch_size)

        cost_vector = []

        model.train()
        samples = random.sample(range(n_batches), n_batches)
        for index in samples:

            x = torch.tensor(x_train[batch_size * index: batch_size * (index + 1)])
            c = torch.tensor(1+u_train[batch_size * index: batch_size * (index + 1)])
            t = torch.tensor(t_train[batch_size * index: batch_size * (index + 1)])
            label = torch.tensor(y_train[batch_size * index: batch_size * (index + 1)])
            l = torch.tensor(l_train[batch_size * index: batch_size * (index + 1)])
            info = torch.tensor(info_train[batch_size * index: batch_size * (index + 1)])
            temp = torch.tensor(temp_train[batch_size * index: batch_size * (index + 1)])

            if(x.shape[0]<batch_size):
                continue
            h = h.data
            arpha_h = arpha_h.data
            final_h = final_h.data
            optimizer.zero_grad()
            out, h, arpha_h, final_h= model(x.to(device).float(), h, c.to(device).float(), arpha_h, final_h, t.to(device).float(), l, info.to(device).float(), temp.to(device).float())

            loss = criterion(out, label.to(device).float())
            loss.backward()
            #torch.nn.utils.clip_grad_value_(model.parameters(), 1)
            optimizer.step()
            cost_vector.append(loss.cpu().data.numpy())
        #scheduler.step()
        #print('Epoch: %d, LR: %f', epoch, scheduler.get_last_lr()[0])

        total_train_cost.append(np.mean(cost_vector))

        model.eval()
        n_batches = int(np.ceil(float(len(x_validate)) / float(batch_size)))
        outputs = np.array([])
        targets = np.array([])
        cost_vector_v = []
        h = model.init_hidden(batch_size)
        arpha_h = model.init_hidden(batch_size)
        final_h = model.init_hidden(batch_size)

        for index in range(n_batches):
            x = torch.tensor(x_validate[batch_size * index: batch_size * (index + 1)])
            c = torch.tensor(1+u_validate[batch_size * index: batch_size * (index + 1)])
            t = torch.tensor(t_validate[batch_size * index: batch_size * (index + 1)])
            label = torch.tensor(y_validate[batch_size * index: batch_size * (index + 1)])
            l = torch.tensor(l_validate[batch_size * index: batch_size * (index + 1)])
            info = torch.tensor(info_validate[batch_size * index: batch_size * (index + 1)])
            temp = torch.tensor(temp_validate[batch_size * index: batch_size * (index + 1)])
            if(x.shape[0]<batch_size):
                continue

            h = h.data
            arpha_h = arpha_h.data
            final_h = final_h.data

            out, h, arpha_h, final_h = model(x.to(device).float(), h, c.to(device).float(), arpha_h, final_h, t.to(device).float(), l, info.to(device).float(), temp.to(device).float())
            loss = criterion(out, label.to(device).float())
            #print(loss)
            cost_vector_v.append(loss.data.cpu().data.numpy())
            out = out>=0.5

            #print(out, label)
            outputs = np.concatenate((outputs, out.data.cpu().detach().numpy().reshape(-1)))
            targets = np.concatenate((targets, label.numpy().reshape(-1)))
        print("Epoch {}/{} Done, Total Loss: {}, Total Valid Loss: {}".format(epoch, EPOCHS, np.mean(cost_vector), np.mean(cost_vector_v)))
        #print(outputs, targets)
        total_valid_cost.append(np.mean(cost_vector_v))
        #print(targets, outputs)
        accuracy = accuracy_score(targets, outputs)
        precision = precision_score(targets, outputs)
        recall = recall_score(targets, outputs)
        f1 = f1_score(targets, outputs)
        roc_auc = roc_auc_score(targets, outputs)
        buf = 'Accuracy:%f, Precision:%f, Recall:%f, F1-score:%f, ROC_AUC:%f' % (
        accuracy, precision, recall, f1, roc_auc)
        print(buf)
        total_accuracy.append(accuracy)
        total_precision.append(precision)
        total_recall.append(recall)
        total_f1.append(f1)
        total_roc_auc.append(roc_auc)

        if np.mean(cost_vector_v) < best_valid_cost :
            best_accuracy = accuracy
            best_epoch = epoch
            best_train_cost = np.mean(cost_vector)
            best_valid_cost = np.mean(cost_vector_v)
            torch.save(model.state_dict(), output_file)
        buf = 'Best Epoch:%d, Accuracy:%f, Train_Cost:%f, Valid_Cost:%f' % (
            best_epoch, best_accuracy, best_train_cost, best_valid_cost)
        print(buf)
    fig, ax = plt.subplots()
    # plot the accuracy
    x = range(len(total_accuracy))  # 50x1 array between 0 and 2*pi
    ax.plot(x, total_accuracy)   # red line without marker
    ax.legend(['accuracy'])
    #plt.show()

    fig, ax = plt.subplots()
    # plot the precision, recall, f1
    ax.plot(x, total_precision, 'b')
    ax.plot(x, total_recall, 'r')
    ax.plot(x, total_f1, 'g')
    ax.legend(['precision', 'recall', 'f1'])
    #plt.show()

    fig, ax = plt.subplots()
    # plot the precision, recall, f1
    ax.plot(x, total_train_cost)
    ax.plot(x, total_valid_cost)
    ax.legend(['loss', 'valid_loss'])
    #plt.show()

    model.load_state_dict(torch.load(output_file))
    return model

def evaluate(model, test_x, test_u, test_t, test_y, test_l, test_info, test_temp):
    model.eval()
    outputs = []
    targets = []

    inp = torch.from_numpy(test_x)
    u_inp = torch.from_numpy(1+test_u)
    t_inp = torch.from_numpy(test_t)
    labs = torch.from_numpy(test_y)
    l_inp = torch.from_numpy(test_l)
    info_inp = torch.from_numpy(test_info)
    temp_inp = torch.from_numpy(test_temp)

    h = model.init_hidden(inp.shape[0])
    arpha_h = model.init_hidden(inp.shape[0])
    final_h = model.init_hidden(inp.shape[0])

    out, h, arpha_h, final_h = model(inp.to(device).float(), h, u_inp.to(device).float(), arpha_h, final_h, t_inp.to(device).float(), l_inp, info_inp.to(device).float(), temp_inp.to(device).float())
    out = out>=0.5
    outputs.append((out.cpu().detach().numpy()).reshape(-1))
    targets.append((labs.numpy()).reshape(-1))

    accuracy = accuracy_score(targets[0], outputs[0])
    precision = precision_score(targets[0], outputs[0])
    recall = recall_score(targets[0], outputs[0])
    f1 = f1_score(targets[0], outputs[0])
    roc_auc = roc_auc_score(targets[0], outputs[0])
    buf = 'Accuracy:%f, Precision:%f, Recall:%f, F1-score:%f, ROC_AUC:%f' % (
    accuracy, precision, recall, f1, roc_auc)
    print(buf)
    confusion = confusion_matrix(targets[0], outputs[0])
    df_cm = pd.DataFrame(confusion, range(2), range(2))
    sn.set(font_scale=1.4) # for label size
    sn.heatmap(df_cm, annot=True, fmt='.20g') # font size
    #plt.show()
    return outputs, targets, accuracy, recall

t_train[336:361]

"""## Train"""

#%% model fit

lr = 0.001
accuracy_l = []
accuracy = 0
recall = 0

seed = 180
print(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
np.random.seed(seed)
random.seed(seed)
torch.backends.cudnn.deterministic = True
gru_model = train(x_train, lr, model_type="GRU")
gru_outputs, targets, accuracy, recall = evaluate(gru_model, x_test, u_test, t_test, y_test, l_test, info_test, temp_test)
accuracy_l.append(accuracy)


